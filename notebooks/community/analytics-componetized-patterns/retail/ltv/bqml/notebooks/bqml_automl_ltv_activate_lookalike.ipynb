{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/notebooks/templates/ai_platform_notebooks_template_hybrid.ipynb\"\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/notebooks/templates/ai_platform_notebooks_template_hybrid.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2VgGzVIlLo2"
      },
      "source": [
        "### Objective\n",
        "\n",
        "Estimate how much an existing customer will spend in the future based on their historical orders to find similar new customers using lookalike features of advertising tools.\n",
        "\n",
        "In this tutorial, you will:\n",
        "- Define how far in the future you want to predict the monetary value of your customers (ex: 3 months)\n",
        "- Use a moving session concept to aggregate multiple inputs and targets per customer (For more details, see the *Create inputs and targets* section of this tutorial).\n",
        "- Use primarly inputs such as Recency, Frequency and Monetary which are common values to use in an LTV context, especially in statistical model due to their distribution patterns.\n",
        "- Accelerate model development by using AutoML from within BigQuery ML.\n",
        "- Predict the monetary value of all existing customers for a predefined period of time in the future.\n",
        "- Use first-party data to extract the most valuable customers email in order to run lookalike campaigns using the [Google Ads API][ads_api]. You can extend the concept to do the same using [Facebook API][fb_api].\n",
        "\n",
        "[ads_api]:https://developers.google.com/adwords/api/docs/samples/python/remarketing#create-and-populate-a-user-list\n",
        "[fb_api]:https://www.facebook.com/business/help/341425252616329?id=2469097953376494\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAeYu9bBlLaZ"
      },
      "source": [
        "### Sales dataset\n",
        "Sales data can be of different forms but generally look like a list of transactions where each record contains at a minimum the following:\n",
        "- a customer reference\n",
        "- a transaction date\n",
        "- a transaction reference\n",
        "- a monetary value\n",
        "\n",
        "Each record usually represents one of the following:\n",
        "- An entire order which contains aggregated values across products for that order. You can find the total order value in the record.\n",
        "- A part of a transaction which contains a unique product, some of its characteristics including SKU and unit price and the quantity ordered. \n",
        "\n",
        "This tutorial uses the latter.\n",
        "\n",
        "You can run this tutorial with your own dataset. The dataset that you provide must meet the following requirements:\n",
        "1. Each row represents a transaction related to a product item and linked to a transaction, a date and a customer. A row can be either a transaction (quantity is > 0) or a return (quantity is < 0)\n",
        "1. Columns must include the following:\n",
        "\n",
        "| Field name | Type | Description |\n",
        "| :-|:-|:-|\n",
        "| customer_id | STRING | First party identifier of the customer.\t |\n",
        "| order_id | STRING | First party identitfier of the order. |\n",
        "| order_date | DATE | Date of the order.  |\n",
        "| product_sku | STRING | First party identitifer of the product. |\n",
        "| qty | INTEGER | Quantity of the product either ordered or returned. |\n",
        "| unit_price | FLOAT | Unit price of the product. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__KXS_qcyNRK"
      },
      "source": [
        "### Customer dataset\n",
        "Customer data often resides in a Customer Relationship Management (CRM). \n",
        "\n",
        "This first party data is key for companies that want to provide a certain level of customer service.\n",
        "\n",
        "This tutorial only uses two columns of the customer dataset:\n",
        "- customer_id to join with the sales data\n",
        "- email to create a marketing list.\n",
        "\n",
        "The public dataset contains other fields that are not relevant for this tutorial and your data might have other fields. This tutorial focuses on an activation based on email addresses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdyWWnIElL0C"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud Platform (GCP):\n",
        "\n",
        "* BigQuery\n",
        "* BigQuery ML\n",
        "* Cloud Storage\n",
        "\n",
        "To learn more about pricing:\n",
        "- Read [BigQuery pricing](https://cloud.google.com/bigquery/docs/pricing)\n",
        "- Read [BigQuery ML pricing](https://cloud.google.com/bigquery-ml/pricing)\n",
        "- Read [Cloud Storage pricing](https://cloud.google.com/storage/pricing)\n",
        "- Use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDn9SREOlXaN"
      },
      "source": [
        "### Terminology\n",
        "- **'Input' transactions**: The set of transactions that the training task uses to create inputs values for the model.\n",
        "- **'Target' transactions**: The set of transactions that the training task uses to create the target value to predict. The target value is an aggregated monetary value per customer for a defined timeline.\n",
        "- **Threshold date**: Date that separates 'Input' transactions from 'Target' transactions per customer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmvu_N0ovY8N"
      },
      "source": [
        "## Setup\n",
        "This step sets up packages, variables, authentication, APIs clients and resources for Google Cloud and Adwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install packages and dependencies\n",
        "Installs libraries, packages and dependencies to run this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyy5Lbnzg5fi"
      },
      "outputs": [],
      "source": [
        "# Install libraries.\n",
        "# The magic cells insures that those libraries can be part of a custom container\n",
        "# if moving the code somewhere else.\n",
        "%pip install -q googleads\n",
        "%pip install -q -U kfp matplotlib Faker --user\n",
        "\n",
        "# Automatically restart kernel after installs\n",
        "# import IPython\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmSgVQ9x4aO0"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouL1aMrvVgnL"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import calendar\n",
        "import hashlib\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.cloud import bigquery\n",
        "from googleads import adwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your GCP project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a GCP project.](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the AI Platform APIs and Compute Engine APIs.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component)\n",
        "\n",
        "4. If you are running this notebook locally, you will need to install [Google Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[YOUR-PROJECT]\"  # @param {type:\"string\"}\n",
        "REGION = \"US\"\n",
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your GCP account\n",
        "If you are using AI Platform Notebooks, you are already authenticated so there is no need to run this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4MuyozBoPv9"
      },
      "source": [
        "### Create a working dataset\n",
        "This tutorial mostly uses BigQuery magic cells where the --params field does not support variables for datasets, tables and column names. \n",
        "\n",
        "This steps hardcode the dataset where all the steps of this tutorial happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGRa9QxXoZMO"
      },
      "outputs": [],
      "source": [
        "! bq show $PROJECT_ID:ltv_ecommerce || bq mk $PROJECT_ID:ltv_ecommerce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6ZRZgqCEPY"
      },
      "source": [
        "### Load example tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjc30TarCHSp"
      },
      "outputs": [],
      "source": [
        "# Loads CRM data\n",
        "!bq load \\\n",
        "  --project_id $PROJECT_ID \\\n",
        "  --skip_leading_rows 1 \\\n",
        "  --max_bad_records 100000 \\\n",
        "  --replace \\\n",
        "  --field_delimiter \",\" \\\n",
        "  --autodetect \\\n",
        "  ltv_ecommerce.00_crm \\\n",
        "  gs://solutions-public-assets/analytics-componentized-patterns/ltv/crm.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe4GT-fWEcUt"
      },
      "outputs": [],
      "source": [
        "# Loads Sales data\n",
        "!bq load \\\n",
        "  --project_id $PROJECT_ID \\\n",
        "  --skip_leading_rows 1 \\\n",
        "  --max_bad_records 100000 \\\n",
        "  --replace \\\n",
        "  --field_delimiter \",\" \\\n",
        "  --autodetect \\\n",
        "  ltv_ecommerce.10_orders \\\n",
        "  gs://solutions-public-assets/analytics-componentized-patterns/ltv/sales_*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVNkkGV8jUN6"
      },
      "source": [
        "### Create clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amZzWHkjjWTi"
      },
      "outputs": [],
      "source": [
        "# BigQuery client\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJB0PYUS7sYw"
      },
      "source": [
        "## [Optional] Match your dataset to template\n",
        "If you use the example data, you can skip this step.\n",
        "\n",
        "This tutorial assumes that you have a dump of your sales data already available in BigQuery located at `[YOUR_PROJECT].[YOUR_DATASET].[YOUR_SOURCE_TABLE]`\n",
        "\n",
        "You are free to adapt the SQL query in the next cell to a SQL statement that transforms your data according to the template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK_wCQt78JjO"
      },
      "outputs": [],
      "source": [
        "%%bigquery --params $MATCH_FIELDS --project $PROJECT_ID\n",
        "\n",
        "CREATE OR REPLACE TABLE `ltv_ecommerce.10_orders` AS (\n",
        "SELECT\n",
        "  CAST(customer_id AS STRING)  AS customer_id,\n",
        "  order_id  AS order_id,\n",
        "  transaction_date AS transaction_date,\n",
        "  product_sku AS product_sku,\n",
        "  qty AS qty,\n",
        "  unit_price AS unit_price\n",
        "FROM\n",
        "  `[YOUR_PROJECT].[YOUR_DATASET].[YOUR_SOURCE_TABLE]`\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9q6jKCM4-v8"
      },
      "source": [
        "## Analyze dataset\n",
        "\n",
        "**Some charts might use a log scale.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEchqKz1HiOZ"
      },
      "source": [
        "#### Quantity\n",
        "This sections shows how to use the BigQuery [ML BUCKETIZE][bucketize] preprocessing function to create buckets of data for quantity and display a log scaled distribution of the `qty` field.\n",
        "\n",
        "[bucketize]:https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions#bucketize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keis4kuj5A71"
      },
      "outputs": [],
      "source": [
        "%%bigquery df_histo_qty --project $PROJECT_ID\n",
        "\n",
        "WITH\n",
        "  min_max AS (\n",
        "  SELECT\n",
        "    MIN(qty) min_qty,\n",
        "    MAX(qty) max_qty,\n",
        "    CEIL((MAX(qty) - MIN(qty)) / 100) step\n",
        "  FROM\n",
        "    `ltv_ecommerce.10_orders` \n",
        ")\n",
        "SELECT\n",
        "  COUNT(1) c,\n",
        "  bucket_same_size AS bucket\n",
        "FROM (\n",
        "  SELECT\n",
        "    -- Creates (1000-100)/100 + 1 buckets of data.\n",
        "    ML.BUCKETIZE(qty, GENERATE_ARRAY(min_qty, max_qty, step)) AS bucket_same_size,\n",
        "    -- Creates custom ranges.\n",
        "    ML.BUCKETIZE(qty, [-1, -1, -2, -3, -4, -5, 0, 1, 2, 3, 4, 5]) AS bucket_specific,\n",
        "  FROM\n",
        "    `ltv_ecommerce.10_orders`, min_max )\n",
        "  # WHERE bucket != \"bin_1\" and bucket != \"bin_2\"\n",
        "GROUP BY\n",
        "  bucket\n",
        "  -- Ohterwise, orders bin_10 before bin_2\n",
        "ORDER BY CAST(SPLIT(bucket, \"_\")[OFFSET(1)] AS INT64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6JqHipXDPto"
      },
      "outputs": [],
      "source": [
        "# Uses a log scale for bucket_same_size.\n",
        "# Can remove the log scale when using bucket_specific.\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.title(\"Log scaled distribution for qty\")\n",
        "hqty = sns.barplot(x=\"bucket\", y=\"c\", data=df_histo_qty)\n",
        "hqty.set_yscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v8wFktnHlA_"
      },
      "source": [
        "#### Unit price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFYqRpzLHnuE"
      },
      "outputs": [],
      "source": [
        "%%bigquery df_histo_unit_price --project $PROJECT_ID\n",
        "\n",
        "WITH\n",
        "  min_max AS (\n",
        "  SELECT\n",
        "    MIN(unit_price) min_unit_price,\n",
        "    MAX(unit_price) max_unit_price,\n",
        "    CEIL((MAX(unit_price) - MIN(unit_price)) / 10) step\n",
        "  FROM\n",
        "    `ltv_ecommerce.10_orders` \n",
        ")\n",
        "\n",
        "SELECT\n",
        "  COUNT(1) c,\n",
        "  bucket_same_size AS bucket\n",
        "FROM (\n",
        "  SELECT\n",
        "    -- Creates (1000-100)/100 + 1 buckets of data.\n",
        "    ML.BUCKETIZE(unit_price, GENERATE_ARRAY(min_unit_price, max_unit_price, step)) AS bucket_same_size,\n",
        "    -- Creates custom ranges.\n",
        "    ML.BUCKETIZE(unit_price, [10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000]) AS bucket_specific,\n",
        "  FROM\n",
        "    `ltv_ecommerce.10_orders`, min_max )\n",
        "  # WHERE bucket != \"bin_1\" and bucket != \"bin_2\"\n",
        "GROUP BY\n",
        "  bucket\n",
        "  -- Ohterwise, orders bin_10 before bin_2\n",
        "ORDER BY CAST(SPLIT(bucket, \"_\")[OFFSET(1)] AS INT64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02bF2qA7HuiU"
      },
      "outputs": [],
      "source": [
        "# Uses a log scale for bucket_same_size.\n",
        "# Can remove the log scale when using bucket_specific.\n",
        "plt.figure(figsize=(12, 5))\n",
        "q = sns.barplot(x=\"bucket\", y=\"c\", data=df_histo_unit_price)\n",
        "q.set_yscale(\"log\")\n",
        "plt.title(\"Log scaled distribution for unit_price\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Ih3v9D6ZGE"
      },
      "source": [
        "## Set parameters for LTV\n",
        "Some parameters useful to run some of the queries in this tutorial:\n",
        "\n",
        "1. **WINDOW_STEP**: How many days between threshold dates.\n",
        "1. **WINDOW_STEP_INITIAL**: How many days between the first order and the first threshold date. A threshold date is when BigQuery computes inputs and targets.\n",
        "1. **WINDOW_LENGTH**: How many days back to use for input transactions. The default value is 0 which means that this tutorial takes all transactions before the threshold date.\n",
        "1. **LENGTH_FUTURE**: How far in the future to predict the monetary value. At every threshold date, BigQuery calculate the target value for all orders that happen LENGTH_FUTURE after the threshold date.\n",
        "1. **MAX_STDV_MONETARY**: Standard deviation of the monetary value per customer. Removes orders per customer that have order values with a greater standard deviation.\n",
        "1. **MAX_STDV_QTY**: Standard deviation of the quantity of products per customer. Removes orders per customer that have product quantity with a greater standard deviation.\n",
        "1. **TOP_LTV_RATIO**: Percentage of top customers that you want to keep for your lookalike activation.\n",
        "\n",
        "You can change those parameters to see how they impact the model especially the parameters related to the window. There is no obvious rule to set values as they depend on how data looks.\n",
        "\n",
        "For example:\n",
        "- If your customers buy multiple times a week, you could try to predict monetary value on a weekly basis.\n",
        "- If you have a lot of data, you can create more windows by decreasing their sizes and possibly the number of days between threshold dates.\n",
        "\n",
        "After multiple trials, this tutorial chose values that provides a decent result for the example dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZGi-Sm5suzM"
      },
      "outputs": [],
      "source": [
        "LTV_PARAMS = {\n",
        "    \"WINDOW_LENGTH\": 0,\n",
        "    \"WINDOW_STEP\": 30,\n",
        "    \"WINDOW_STEP_INITIAL\": 90,\n",
        "    \"LENGTH_FUTURE\": 30,\n",
        "    \"MAX_STDV_MONETARY\": 500,\n",
        "    \"MAX_STDV_QTY\": 100,\n",
        "    \"TOP_LTV_RATIO\": 0.2,\n",
        "}\n",
        "LTV_PARAMS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhOH4XM52otf"
      },
      "source": [
        "## Aggregate per day per customer\n",
        "This query aggregates all orders per day per customer. This is useful if:\n",
        "- Your database has multiple records for one order, for example if each record represent a product in a transaction, which is the case in this tutorial.\n",
        "- Customers bought multiple times during one day. This guide is based on LTV per day so no need to keep hourly records (unless you decided to use that data as a feature).\n",
        "\n",
        "This query also\n",
        "1. Creates inputs related to returns.\n",
        "1. Removes orders with outliers per customer. This means that high spending customers remain in the dataset but orders that seems unusual for a unique customers are filtered out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0sb0KFeik5M"
      },
      "outputs": [],
      "source": [
        "%%bigquery --params $LTV_PARAMS --project $PROJECT_ID\n",
        "\n",
        "DECLARE MAX_STDV_MONETARY INT64 DEFAULT @MAX_STDV_MONETARY;\n",
        "DECLARE MAX_STDV_QTY INT64 DEFAULT @MAX_STDV_QTY;\n",
        "\n",
        "CREATE OR REPLACE TABLE `ltv_ecommerce.20_aggred` AS\n",
        "SELECT\n",
        "  customer_id,\n",
        "  order_day,\n",
        "  ROUND(day_value_after_returns, 2) AS value,\n",
        "  day_qty_after_returns as qty_articles,\n",
        "  day_num_returns AS num_returns,\n",
        "  CEIL(avg_time_to_return) AS time_to_return\n",
        "FROM (\n",
        "  SELECT\n",
        "    customer_id,\n",
        "    order_day,\n",
        "    SUM(order_value_after_returns) AS day_value_after_returns,\n",
        "    STDDEV(SUM(order_value_after_returns)) OVER(PARTITION BY customer_id ORDER BY SUM(order_value_after_returns)) AS stdv_value,\n",
        "    SUM(order_qty_after_returns) AS day_qty_after_returns,\n",
        "    STDDEV(SUM(order_qty_after_returns)) OVER(PARTITION BY customer_id ORDER BY SUM(order_qty_after_returns)) AS stdv_qty,\n",
        "    CASE\n",
        "      WHEN MIN(order_min_qty) < 0 THEN count(1)\n",
        "      ELSE 0\n",
        "    END AS day_num_returns,\n",
        "    CASE\n",
        "      WHEN MIN(order_min_qty) < 0 THEN AVG(time_to_return)\n",
        "      ELSE NULL\n",
        "    END AS avg_time_to_return\n",
        "  FROM (\n",
        "    SELECT \n",
        "      customer_id,\n",
        "      order_id,\n",
        "      -- Gives the order date vs return(s) dates.\n",
        "      MIN(transaction_date) AS order_day,\n",
        "      MAX(transaction_date) AS return_final_day,\n",
        "      DATE_DIFF(MAX(transaction_date), MIN(transaction_date), DAY) AS time_to_return,\n",
        "      -- Aggregates all products in the order \n",
        "      -- and all products returned later.\n",
        "      SUM(qty * unit_price) AS order_value_after_returns,\n",
        "      SUM(qty) AS order_qty_after_returns,\n",
        "      -- If negative, order has qty return(s).\n",
        "      MIN(qty) order_min_qty\n",
        "    FROM \n",
        "      `ltv_ecommerce.10_orders`\n",
        "    GROUP BY\n",
        "      customer_id,\n",
        "      order_id)\n",
        "  GROUP BY\n",
        "    customer_id,\n",
        "    order_day)\n",
        "WHERE\n",
        "  -- [Optional] Remove dates with outliers per a customer.\n",
        "  (stdv_value < MAX_STDV_MONETARY\n",
        "    OR stdv_value IS NULL) AND\n",
        "  (stdv_qty < MAX_STDV_QTY\n",
        "    OR stdv_qty IS NULL);\n",
        "\n",
        "\n",
        "SELECT * FROM `ltv_ecommerce.20_aggred` LIMIT 5;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq0WFrHqGgjE"
      },
      "source": [
        "## Check distributions\n",
        "This tutorial does minimum data cleansing and focuses mostly on transforming a list of transactions into workable inputs for the model.\n",
        "\n",
        "This section checks that data is generally usable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cYheRxJIYjs"
      },
      "source": [
        "### Per date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulyzgY2RGg0N"
      },
      "outputs": [],
      "source": [
        "%%bigquery df_dist_dates --project $PROJECT_ID\n",
        "\n",
        "SELECT count(1) c, SUBSTR(CAST(order_day AS STRING), 0, 7) as yyyy_mm\n",
        "FROM `ltv_ecommerce.20_aggred`\n",
        "WHERE qty_articles > 0\n",
        "GROUP BY yyyy_mm \n",
        "ORDER BY yyyy_mm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgSVTR0pIxpV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "sns.barplot(x=\"yyyy_mm\", y=\"c\", data=df_dist_dates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_85oPxL4zqQF"
      },
      "source": [
        "Orders are quite well distributed across the year despite a lower number in the early days of the dataset. You can keep this in mind when choosing a value for `WINDOW_STEP_INITIAL`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opxsPUhZJQtY"
      },
      "source": [
        "### Per customer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giPBoy4dJQ4O"
      },
      "outputs": [],
      "source": [
        "%%bigquery df_dist_customers --params $LTV_PARAMS --project $PROJECT_ID\n",
        "\n",
        "SELECT customer_id, count(1) c\n",
        "FROM `ltv_ecommerce.20_aggred`\n",
        "GROUP BY customer_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jnDQRtiJQ7r"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "sns.distplot(df_dist_customers[\"c\"], hist_kws=dict(ec=\"k\"), kde=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzvPvPvl0WHD"
      },
      "source": [
        "The number of transactions per customer is distributed across a few discrete values with no clear outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEkWNb1Z11GE"
      },
      "source": [
        "### Per quantity\n",
        "This section looks at the general distribution of the number of articles per orders and check if there are some outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ak9z1us2Da5"
      },
      "outputs": [],
      "source": [
        "%%bigquery df_dist_qty --params $LTV_PARAMS --project $PROJECT_ID\n",
        "\n",
        "SELECT qty_articles, count(1) c\n",
        "FROM `ltv_ecommerce.20_aggred`\n",
        "GROUP BY qty_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlF-3blV2M_N"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "sns.distplot(df_dist_qty[\"qty_articles\"], hist_kws=dict(ec=\"k\"), kde=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK-0HMq63XZD"
      },
      "source": [
        "A few customers seems to have quite large quantities in their orders but the distribution is generally healthy. \n",
        "\n",
        "They could be weird behavior for the same customers but the Standard Deviation filters should minimize the risk. \n",
        "\n",
        "Analyzing those few outliers can be part of additional data preparation work that can help improve your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFm1_oKJkVZ"
      },
      "source": [
        "### Per value\n",
        "This section shows that there are quite a few outliers 'outliers' that spend way more than others. We want to ignore them when creating the RFM values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnMBqS7uhooL"
      },
      "outputs": [],
      "source": [
        "%%bigquery df_dist_values --params $LTV_PARAMS --project $PROJECT_ID\n",
        "\n",
        "SELECT value\n",
        "FROM `ltv_ecommerce.20_aggred`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rO6XsZkd-bh"
      },
      "outputs": [],
      "source": [
        "axv = sns.violinplot(x=df_dist_values[\"value\"])\n",
        "axv.set_xlim(-200, 3500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uY8l0Os51xW"
      },
      "source": [
        "The distribution shows a few outliers that you could investigate to improve the base model that you create in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxrNfMNMUifK"
      },
      "source": [
        "## Create inputs and targets for the ML model\n",
        "The goal of this tutorial is to predict *How much each customer is going to spend in the next N days knowing their transactions history.*\n",
        "\n",
        "Because all customers have different buying behavior over a specific period of time, this tutorial create multiple records per customer by:\n",
        "\n",
        "1. Moving a threshold date by `WINDOW_STEP` over the dataset to create input and targets.\n",
        "1. Aggregates input data per customer using transactions between `WINDOW_START` and `THRESHOLD_DATE` threshold date.\n",
        "1. Aggregates the monetary value of the target transactions whose dates are between `THRESHOLD_DATE` and `THRESHOLD_DATE + LENGTH_FUTURE`.\n",
        "\n",
        "Based on the size of your dataset and what you are trying to predict, you can update those values as needed in the *Set parameters for LTV* section.\n",
        "\n",
        "This tutorial uses all historical data to predict the monetary value of its customers in the next 30 (`LENGTH_FUTURE`) days with a dataset that contains inputs/target value calculated every 30 days (`WINDOW_STEP`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFjSNi_lFgPc"
      },
      "source": [
        "### Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tY96qAzgXjh"
      },
      "outputs": [],
      "source": [
        "%%bigquery --params $LTV_PARAMS --project $PROJECT_ID\n",
        "\n",
        "-- Lenght are number of days\n",
        "--\n",
        "-- Date of the first order in the dataset.\n",
        "DECLARE MIN_DATE DATE;                                          \n",
        "-- Date of the final order in the dataset.\n",
        "DECLARE MAX_DATE DATE;                                          \n",
        "-- Date that separates inputs orders from target transactions.\n",
        "DECLARE THRESHOLD_DATE DATE;                                    \n",
        "-- How many days back for inputs transactions. 0 means from the start.\n",
        "DECLARE WINDOW_LENGTH INT64 DEFAULT @WINDOW_LENGTH;\n",
        "-- Date at which an input transactions window starts.\n",
        "DECLARE WINDOW_START DATE;\n",
        "-- How many days between thresholds.\n",
        "DECLARE WINDOW_STEP INT64 DEFAULT @WINDOW_STEP;                 \n",
        "-- How many days for the first window.\n",
        "DECLARE WINDOW_STEP_INITIAL INT64 DEFAULT @WINDOW_STEP_INITIAL; \n",
        "-- Index of the window being run.\n",
        "DECLARE STEP INT64 DEFAULT 1;                                   \n",
        "-- How many days to predict for.\n",
        "DECLARE LENGTH_FUTURE INT64 DEFAULT @LENGTH_FUTURE;                            \n",
        "\n",
        "SET (MIN_DATE, MAX_DATE) = (\n",
        "  SELECT AS STRUCT \n",
        "    MIN(order_day) AS min_days,\n",
        "    MAX(order_day) AS max_days\n",
        "  FROM\n",
        "    `ltv_ecommerce.20_aggred`\n",
        ");\n",
        "\n",
        "SET THRESHOLD_DATE = MIN_DATE;\n",
        "\n",
        "-- For more information about the features of this table,\n",
        "-- see https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/utils.py#L246\n",
        "-- and https://cloud.google.com/solutions/machine-learning/clv-prediction-with-offline-training-train#aggregating_data\n",
        "CREATE OR REPLACE TABLE ltv_ecommerce.30_featured\n",
        "(\n",
        "  -- dataset STRING,\n",
        "  customer_id STRING,\n",
        "  monetary FLOAT64,\n",
        "  frequency INT64,\n",
        "  recency INT64,\n",
        "  T INT64,\n",
        "  time_between FLOAT64,\n",
        "  avg_basket_value FLOAT64,\n",
        "  avg_basket_size FLOAT64,\n",
        "  has_returns STRING,\n",
        "  avg_time_to_return FLOAT64,\n",
        "  num_returns INT64,\n",
        "  -- threshold DATE,\n",
        "  -- step INT64,\n",
        "  target_monetary FLOAT64,\n",
        ");\n",
        "\n",
        "LOOP\n",
        "  -- Can choose a longer original window in case \n",
        "  -- there were not many orders in the early days.\n",
        "  IF STEP = 1 THEN\n",
        "    SET THRESHOLD_DATE = DATE_ADD(THRESHOLD_DATE, INTERVAL WINDOW_STEP_INITIAL DAY); \n",
        "  ELSE\n",
        "    SET THRESHOLD_DATE = DATE_ADD(THRESHOLD_DATE, INTERVAL WINDOW_STEP DAY);\n",
        "  END IF;\n",
        "  SET STEP = STEP + 1;\n",
        "\n",
        "  IF THRESHOLD_DATE >= DATE_SUB(MAX_DATE, INTERVAL (WINDOW_STEP) DAY) THEN\n",
        "    LEAVE;\n",
        "  END IF;\n",
        "\n",
        "  -- Takes all transactions before the threshold date unless you decide\n",
        "  -- to use a different window lenght to test model performance.\n",
        "  IF WINDOW_LENGTH != 0 THEN\n",
        "    SET WINDOW_START = DATE_SUB(THRESHOLD_DATE, INTERVAL WINDOW_LENGTH DAY);\n",
        "  ELSE\n",
        "    SET WINDOW_START = MIN_DATE;\n",
        "  END IF;\n",
        "\n",
        "  INSERT ltv_ecommerce.30_featured\n",
        "  SELECT\n",
        "    -- CASE\n",
        "    --   WHEN THRESHOLD_DATE <= DATE_SUB(MAX_DATE, INTERVAL LENGTH_FUTURE DAY) THEN 'UNASSIGNED'\n",
        "    --   ELSE 'TEST'\n",
        "    -- END AS dataset,\n",
        "    CAST(tf.customer_id AS STRING),\n",
        "    ROUND(tf.monetary_orders, 2) AS monetary,\n",
        "    tf.cnt_orders AS frequency,\n",
        "    tf.recency,\n",
        "    tf.T,\n",
        "    ROUND(tf.recency/cnt_orders, 2) AS time_between,\n",
        "    ROUND(tf.avg_basket_value, 2) AS avg_basket_value,\n",
        "    ROUND(tf.avg_basket_size, 2) AS avg_basket_size,\n",
        "    has_returns,\n",
        "    CEIL(avg_time_to_return) AS avg_time_to_return,\n",
        "    num_returns,\n",
        "    -- THRESHOLD_DATE AS threshold,\n",
        "    -- STEP - 1 AS step,\n",
        "    ROUND(tt.target_monetary, 2) AS target_monetary,\n",
        "  FROM (\n",
        "      -- This SELECT uses only data before THRESHOLD_DATE to make features.\n",
        "      SELECT\n",
        "        customer_id,\n",
        "        SUM(value) AS monetary_orders,\n",
        "        DATE_DIFF(MAX(order_day), MIN(order_day), DAY) AS recency,\n",
        "        DATE_DIFF(THRESHOLD_DATE, MIN(order_day), DAY) AS T,\n",
        "        COUNT(DISTINCT order_day) AS cnt_orders,\n",
        "        AVG(qty_articles) avg_basket_size,\n",
        "        AVG(value) avg_basket_value,\n",
        "        CASE\n",
        "          WHEN SUM(num_returns) > 0 THEN 'y'\n",
        "          ELSE 'n'\n",
        "        END AS has_returns,\n",
        "        AVG(time_to_return) avg_time_to_return,\n",
        "        THRESHOLD_DATE AS threshold,\n",
        "        SUM(num_returns) num_returns,\n",
        "      FROM\n",
        "        `ltv_ecommerce.20_aggred`\n",
        "      WHERE\n",
        "        order_day <= THRESHOLD_DATE AND\n",
        "        order_day >= WINDOW_START\n",
        "      GROUP BY\n",
        "        customer_id\n",
        "    ) tf\n",
        "  INNER JOIN (\n",
        "    -- This SELECT uses all data after threshold as target.\n",
        "    SELECT\n",
        "      customer_id,\n",
        "      SUM(value) target_monetary\n",
        "    FROM\n",
        "      `ltv_ecommerce.20_aggred`\n",
        "    WHERE\n",
        "      order_day <= DATE_ADD(THRESHOLD_DATE, INTERVAL LENGTH_FUTURE DAY)\n",
        "      -- Overall value is similar to predicting only what's after threshold.\n",
        "      -- and the prediction performs better. We can substract later.\n",
        "      -- AND order_day > THRESHOLD_DATE\n",
        "    GROUP BY\n",
        "      customer_id) tt\n",
        "  ON\n",
        "    tf.customer_id = tt.customer_id;\n",
        "\n",
        "END LOOP;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPwha05IFivK"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kNnLuWt-HBb"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID\n",
        "\n",
        "-- Shows all data for a specific customer and some other random records.\n",
        "SELECT * FROM `ltv_ecommerce.30_featured` WHERE customer_id = \"10\"\n",
        "UNION ALL\n",
        "(SELECT * FROM `ltv_ecommerce.30_featured` LIMIT 5)\n",
        "ORDER BY customer_id, frequency, T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qKPX7HV2VoP"
      },
      "outputs": [],
      "source": [
        "%%bigquery df_featured --project $PROJECT_ID\n",
        "\n",
        "ltv_ecommerce.30_featured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuG-eO8qMJdo"
      },
      "outputs": [],
      "source": [
        "df_featured.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBr78kKG2j-3"
      },
      "outputs": [],
      "source": [
        "# Display distribution for all columns that are numerical (but will still ignore the categorical ones like day of the week)\n",
        "valid_column_names = [\n",
        "    key\n",
        "    for key in dict(df_featured.dtypes)\n",
        "    if dict(df_featured.dtypes)[key] in [\"float64\", \"int64\"]\n",
        "]\n",
        "NUM_COLS = 5\n",
        "NUM_ROWS = math.ceil(int(len(valid_column_names)) / NUM_COLS)\n",
        "\n",
        "fig, axs = plt.subplots(nrows=NUM_ROWS, ncols=NUM_COLS, figsize=(25, 7))\n",
        "\n",
        "for idx, cname in enumerate(valid_column_names):\n",
        "    x = int(idx / NUM_COLS)\n",
        "    y = idx % NUM_COLS\n",
        "    sns.violinplot(df_featured[cname], ax=axs[x, y], label=cname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-vSwvNNBKgZ"
      },
      "source": [
        "Seems like for most values, there is a long tail of records. This is something that might required additional feature preparation even if AutoML already provides some automatic engineering. You can investigate this if you want to improve the base model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g8ZoZcJfxoJ"
      },
      "source": [
        "## Train the model\n",
        "This tutorial uses an AutoML regressor to predict the continuous value of target_monetary.\n",
        "\n",
        "With a non-AutoML model, you would generally need to:\n",
        "1. Apply common ML patterns such as normalization or clipping.\n",
        "1. Split data in two to three datasets for training, evaluating and testing.\n",
        "\n",
        "AutoML lets you split your data:\n",
        "- Manually using a column with a name for each split.\n",
        "- Manually using a column that defines a time\n",
        "- Automatically\n",
        "\n",
        "This tutorial uses the latter option where AutoML automatically assigns each row to a split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyjyGOicfx-L"
      },
      "outputs": [],
      "source": [
        "# You can run this query using the magic cell but the cell would run for hours.\n",
        "# Although stopping the cell would not stop the query, using the Python client\n",
        "# also enables you to add a custom parameter for the model name.\n",
        "suffix_now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "train_model_jobid = f\"train_model_{suffix_now}\"\n",
        "\n",
        "train_model_sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `ltv_ecommerce.model_tutorial_{suffix_now}`\n",
        "       OPTIONS(MODEL_TYPE=\"AUTOML_REGRESSOR\",\n",
        "               INPUT_LABEL_COLS=[\"target_monetary\"],\n",
        "               OPTIMIZATION_OBJECTIVE=\"MINIMIZE_MAE\")\n",
        "AS SELECT \n",
        "  * EXCEPT(customer_id)\n",
        "FROM \n",
        "  `ltv_ecommerce.30_featured`\n",
        "\"\"\"\n",
        "\n",
        "bq_client.query(train_model_sql, job_id=train_model_jobid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1rW-h3wQW80"
      },
      "source": [
        "This is an example of a model evaluation\n",
        "\n",
        "![Model Evaluation](https://storage.googleapis.com/solutions-public-assets/analytics-componentized-patterns/ltv/model_evaluation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJnwrWjT_4W7"
      },
      "source": [
        "## Predict LTV\n",
        "Predicts LTV for all customers. It uses the overall monetary value for each customer to predict a future one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_nmt4DuIVzL"
      },
      "outputs": [],
      "source": [
        "%%bigquery --params $LTV_PARAMS --project $PROJECT_ID\n",
        "\n",
        "-- TODO(developer): \n",
        "-- 1. Update the model name to the one you want to use.\n",
        "-- 2. Update the table where to output predictions.\n",
        "\n",
        "-- How many days back for inputs transactions. 0 means from the start.\n",
        "DECLARE WINDOW_LENGTH INT64 DEFAULT @WINDOW_LENGTH;\n",
        "-- Date at which an input transactions window starts.\n",
        "DECLARE WINDOW_START DATE;\n",
        "\n",
        "-- Date of the first transaction in the dataset.\n",
        "DECLARE MIN_DATE DATE;                                          \n",
        "-- Date of the final transaction in the dataset.\n",
        "DECLARE MAX_DATE DATE; \n",
        "-- Date from which you want to predict.\n",
        "DECLARE PREDICT_FROM_DATE DATE;\n",
        "\n",
        "SET (MIN_DATE, MAX_DATE) = (\n",
        "  SELECT AS STRUCT \n",
        "    MIN(order_day) AS min_days,\n",
        "    MAX(order_day) AS max_days\n",
        "  FROM\n",
        "    `ltv_ecommerce.20_aggred`\n",
        ");\n",
        "\n",
        "-- You can set any date here. In production, it is generally today.\n",
        "SET PREDICT_FROM_DATE = MAX_DATE;\n",
        "\n",
        "IF WINDOW_LENGTH != 0 THEN\n",
        "  SET WINDOW_START = DATE_SUB(PREDICT_FROM_DATE, INTERVAL WINDOW_LENGTH DAY);\n",
        "ELSE\n",
        "  SET WINDOW_START = MIN_DATE;\n",
        "END IF;\n",
        "\n",
        "CREATE OR REPLACE TABLE `ltv_ecommerce.predictions_tutorial`\n",
        "AS (\n",
        "SELECT\n",
        "  customer_id,\n",
        "  monetary AS monetary_so_far,\n",
        "  ROUND(predicted_target_monetary, 2) AS monetary_predicted,\n",
        "  ROUND(predicted_target_monetary - monetary, 2) AS monetary_future\n",
        "FROM\n",
        "  ML.PREDICT(\n",
        "    -- /!\\ Set your model name here.\n",
        "    MODEL ltv_ecommerce.model_tutorial_YYYYMMDD,\n",
        "    (\n",
        "      SELECT\n",
        "        customer_id,\n",
        "        ROUND(monetary_orders, 2) AS monetary,\n",
        "        cnt_orders AS frequency,\n",
        "        recency,\n",
        "        T,\n",
        "        ROUND(recency/cnt_orders, 2) AS time_between,\n",
        "        ROUND(avg_basket_value, 2) AS avg_basket_value,\n",
        "        ROUND(avg_basket_size, 2) AS avg_basket_size,\n",
        "        has_returns,\n",
        "        CEIL(avg_time_to_return) AS avg_time_to_return,\n",
        "        num_returns\n",
        "      FROM (\n",
        "        SELECT\n",
        "          customer_id,\n",
        "          SUM(value) AS monetary_orders,\n",
        "          DATE_DIFF(MAX(order_day), MIN(order_day), DAY) AS recency,\n",
        "          DATE_DIFF(PREDICT_FROM_DATE, MIN(order_day), DAY) AS T,\n",
        "          COUNT(DISTINCT order_day) AS cnt_orders,\n",
        "          AVG(qty_articles) avg_basket_size,\n",
        "          AVG(value) avg_basket_value,\n",
        "          CASE\n",
        "            WHEN SUM(num_returns) > 0 THEN 'y'\n",
        "            ELSE 'n'\n",
        "          END AS has_returns,\n",
        "          AVG(time_to_return) avg_time_to_return,\n",
        "          SUM(num_returns) num_returns,\n",
        "        FROM\n",
        "          `ltv_ecommerce.20_aggred`\n",
        "        WHERE\n",
        "          order_day <= PREDICT_FROM_DATE AND\n",
        "          order_day >= WINDOW_START\n",
        "        GROUP BY\n",
        "          customer_id\n",
        "      )\n",
        "    )\n",
        "  )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQVtTFK0Q7fF"
      },
      "outputs": [],
      "source": [
        "%%bigquery df_predictions --project $PROJECT_ID\n",
        "\n",
        "ltv_ecommerce.predictions_windowed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPSdMaTdRCzz"
      },
      "outputs": [],
      "source": [
        "df_predictions.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhy9Sb2Hwmox"
      },
      "outputs": [],
      "source": [
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "fig = plt.figure(constrained_layout=True, figsize=(15, 5))\n",
        "gs = GridSpec(2, 2, figure=fig)\n",
        "\n",
        "sns.set(font_scale=1)\n",
        "plt.tick_params(axis=\"x\", labelsize=14)\n",
        "\n",
        "ax0 = plt.subplot(gs.new_subplotspec((0, 0), colspan=1))\n",
        "ax1 = plt.subplot(gs.new_subplotspec((0, 1), colspan=1))\n",
        "ax2 = plt.subplot(gs.new_subplotspec((1, 0), colspan=2))\n",
        "\n",
        "sns.violinplot(df_predictions[\"monetary_so_far\"], ax=ax0, label=\"monetary_so_far\")\n",
        "sns.violinplot(df_predictions[\"monetary_predicted\"], ax=ax1, label=\"monetary_predicted\")\n",
        "sns.violinplot(df_predictions[\"monetary_future\"], ax=ax2, label=\"monetary_future\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6YQTMvX5Q08"
      },
      "source": [
        "The monetary distribution analysis shows small monetary amounts for the next month compare to the overall historical value. The difference is about 3 to 4 orders of magnitude.\n",
        "\n",
        "One reason is that the model is trained to predict the value for the next month (LENGTH_FUTURE = 30). \n",
        "\n",
        "You can play around with that value to train and predict for the next quarter for example (LENGTH_FUTURE = 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD8paLvr7Kvb"
      },
      "source": [
        "## Activation\n",
        "This part shows how to activate on [Google Ads][lookalike_adwords] using similar audience.\n",
        "\n",
        "You can follow a similar process for [Facebook][lookalike_facebook] for example\n",
        "\n",
        "[lookalike_adwords]: https://developers.google.com/adwords/api/docs/guides/remarketing#customer_match_with_email_address_address_or_user_id\n",
        "[lookalike_facebook]: https://www.facebook.com/business/help/170456843145568?id=2469097953376494"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmK61nL6_KbC"
      },
      "source": [
        "### Extract top customers\n",
        "\n",
        "This step extracts the top 20% customers with the highest **future** monetary value and join with a CRM table to get their email. \n",
        "\n",
        "The prediction used the overall monetary value but in this use case, we are looking at the most valuable in the future. You can modify the PERCENT_RANK to use another KPI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4QBQflQ_1NU"
      },
      "outputs": [],
      "source": [
        "%%bigquery df_top_ltv --params $LTV_PARAMS --project $PROJECT_ID\n",
        "\n",
        "DECLARE TOP_LTV_RATIO FLOAT64 DEFAULT @TOP_LTV_RATIO;\n",
        "\n",
        "SELECT\n",
        "  p.customer_id,\n",
        "  monetary_future,\n",
        "  c.email AS email\n",
        "FROM (\n",
        "  SELECT\n",
        "    customer_id,\n",
        "    monetary_future,\n",
        "    PERCENT_RANK() OVER (ORDER BY monetary_future DESC) AS percent_rank_monetary\n",
        "  FROM\n",
        "    `ltv_ecommerce.predictions_windowed` ) p\n",
        "-- This creates fake emails. You need to join with your own CRM table.\n",
        "INNER JOIN (\n",
        "  SELECT\n",
        "    customer_id,\n",
        "    email\n",
        "  FROM\n",
        "    `ltv_ecommerce.00_crm` ) c\n",
        "ON\n",
        "  p.customer_id = CAST(c.customer_id AS STRING)\n",
        "WHERE\n",
        "  -- Decides the size of your list of emails. For similar-audience use cases \n",
        "  -- where you need to find a minimum of matching emails, 20% should provide\n",
        "  -- enough potential emails.\n",
        "  percent_rank_monetary <= TOP_LTV_RATIO\n",
        "ORDER BY monetary_future DESC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOlGSv1eHFGh"
      },
      "outputs": [],
      "source": [
        "df_top_ltv.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KtOjoRTkCjF"
      },
      "outputs": [],
      "source": [
        "# Shows distribution of the predicted monetary value for the top LTV customers.\n",
        "print(df_top_ltv.describe())\n",
        "\n",
        "fig, axs = plt.subplots()\n",
        "sns.set(font_scale=1.2)\n",
        "sns.distplot(df_top_ltv[\"monetary_future\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7yvUDKWgZ5_"
      },
      "source": [
        "### Setup Adwords client\n",
        "Creates the configuration YAML file for the Google Ads client. You need to:\n",
        "1. Create Client ID and Secret using the Cloud Console\n",
        "2. Follow [these steps](https://developers.google.com/google-ads/api/docs/client-libs/python/oauth-installed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVDjGJ87hhQG"
      },
      "outputs": [],
      "source": [
        "# Sets your variables.\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import files\n",
        "\n",
        "ADWORDS_FILE = \"/tmp/adwords.yaml\"\n",
        "\n",
        "DEVELOPER_TOKEN = \"[YOUR_DEVELOPER_TOKEN]\"\n",
        "OAUTH_2_CLIENT_ID = \"[YOUR_OAUTH_2_CLIENT_ID]\"\n",
        "CLIENT_SECRET = \"[YOUR_CLIENT_SECRET]\"\n",
        "REFRESH_TOKEN = \"[YOUR_REFRESH_TOKEN]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIm9PUspgco4"
      },
      "outputs": [],
      "source": [
        "# Creates a local YAML file\n",
        "adwords_content = f\"\"\"\n",
        "# AdWordsClient configurations\n",
        "adwords:\n",
        "  #############################################################################\n",
        "  # Required Fields                                                           #\n",
        "  #############################################################################\n",
        "  developer_token: {DEVELOPER_TOKEN}\n",
        "  #############################################################################\n",
        "  # Optional Fields                                                           #\n",
        "  #############################################################################\n",
        "  # client_customer_id: INSERT_CLIENT_CUSTOMER_ID_HERE\n",
        "  # user_agent: INSERT_USER_AGENT_HERE\n",
        "  # partial_failure: True\n",
        "  # validate_only: True\n",
        "  #############################################################################\n",
        "  # OAuth2 Configuration                                                      #\n",
        "  # Below you may provide credentials for either the installed application or #\n",
        "  # service account flows. Remove or comment the lines for the flow you're    #\n",
        "  # not using.                                                                #\n",
        "  #############################################################################\n",
        "  # The following values configure the client for the installed application\n",
        "  # flow.\n",
        "  client_id: {OAUTH_2_CLIENT_ID}\n",
        "  client_secret: {CLIENT_SECRET}\n",
        "  refresh_token: {REFRESH_TOKEN}\n",
        "  # The following values configure the client for the service account flow.\n",
        "  # path_to_private_key_file: INSERT_PATH_TO_JSON_KEY_FILE_HERE\n",
        "  # delegated_account: INSERT_DOMAIN_WIDE_DELEGATION_ACCOUNT\n",
        "  #############################################################################\n",
        "  # ReportDownloader Headers                                                  #\n",
        "  # Below you may specify boolean values for optional headers that will be    #\n",
        "  # applied to all requests made by the ReportDownloader utility by default.  #\n",
        "  #############################################################################\n",
        "  # report_downloader_headers:\n",
        "    # skip_report_header: False\n",
        "    # skip_column_header: False\n",
        "    # skip_report_summary: False\n",
        "    # use_raw_enum_values: False\n",
        "\n",
        "\n",
        "# AdManagerClient configurations\n",
        "ad_manager:\n",
        "  #############################################################################\n",
        "  # Required Fields                                                           #\n",
        "  #############################################################################\n",
        "  application_name: INSERT_APPLICATION_NAME_HERE\n",
        "  #############################################################################\n",
        "  # Optional Fields                                                           #\n",
        "  #############################################################################\n",
        "  # The network_code is required for all services except NetworkService:\n",
        "  # network_code: INSERT_NETWORK_CODE_HERE\n",
        "  # delegated_account: INSERT_DOMAIN_WIDE_DELEGATION_ACCOUNT\n",
        "  #############################################################################\n",
        "  # OAuth2 Configuration                                                      #\n",
        "  # Below you may provide credentials for either the installed application or #\n",
        "  # service account (recommended) flows. Remove or comment the lines for the  #\n",
        "  # flow you're not using.                                                    #\n",
        "  #############################################################################\n",
        "  # The following values configure the client for the service account flow.\n",
        "  path_to_private_key_file: INSERT_PATH_TO_JSON_KEY_FILE_HERE\n",
        "  # delegated_account: INSERT_DOMAIN_WIDE_DELEGATION_ACCOUNT\n",
        "  # The following values configure the client for the installed application\n",
        "  # flow.\n",
        "  # client_id: INSERT_OAUTH_2_CLIENT_ID_HERE\n",
        "  # client_secret: INSERT_CLIENT_SECRET_HERE\n",
        "  # refresh_token: INSERT_REFRESH_TOKEN_HERE\n",
        "\n",
        "\n",
        "# Common configurations:\n",
        "###############################################################################\n",
        "# Compression (optional)                                                      #\n",
        "# Below you may specify whether to accept and automatically decompress gzip   #\n",
        "# encoded SOAP requests. By default, gzip compression is not enabled.         #\n",
        "###############################################################################\n",
        "# enable_compression: False\n",
        "###############################################################################\n",
        "# Logging configuration (optional)                                            #\n",
        "# Below you may specify the logging configuration. This will be provided as   #\n",
        "# an input to logging.config.dictConfig.                                      #\n",
        "###############################################################################\n",
        "# logging:\n",
        "  # version: 1\n",
        "  # disable_existing_loggers: False\n",
        "  # formatters:\n",
        "    # default_fmt:\n",
        "      # format: ext://googleads.util.LOGGER_FORMAT\n",
        "  # handlers:\n",
        "    # default_handler:\n",
        "      # class: logging.StreamHandler\n",
        "      # formatter: default_fmt\n",
        "      # level: INFO\n",
        "  # loggers:\n",
        "    # Configure root logger\n",
        "    # \"\":\n",
        "      # handlers: [default_handler]\n",
        "      # level: INFO\n",
        "###############################################################################\n",
        "# Proxy configurations (optional)                                             #\n",
        "# Below you may specify an HTTP or HTTPS Proxy to be used when making API     #\n",
        "# requests. Note: You must specify the scheme used for the proxy endpoint.    #\n",
        "#                                                                             #\n",
        "# For additional information on configuring these values, see:                #\n",
        "# http://docs.python-requests.org/en/master/user/advanced/#proxies            #\n",
        "###############################################################################\n",
        "# proxy_config:\n",
        "  # http: INSERT_HTTP_PROXY_URI_HERE\n",
        "  # https: INSERT_HTTPS_PROXY_URI_HERE\n",
        "  # If specified, the given cafile will only be used if certificate validation\n",
        "  # is not disabled.\n",
        "  # cafile: INSERT_PATH_HERE\n",
        "  # disable_certificate_validation: False\n",
        "################################################################################\n",
        "# Utilities Included (optional)                                                #\n",
        "# Below you may specify whether the library will include utilities used in the #\n",
        "# user agent. By default, the library will include utilities used in the user  #\n",
        "# agent.                                                                       #\n",
        "################################################################################\n",
        "# include_utilities_in_user_agent: True\n",
        "################################################################################\n",
        "# Custom HTTP headers (optional)                                               #\n",
        "# Specify one or more custom headers to pass along with all requests to        #\n",
        "# the API.                                                                     #\n",
        "################################################################################\n",
        "# custom_http_headers:\n",
        "#   X-My-Header: 'content'\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX99FqKHh4gb"
      },
      "outputs": [],
      "source": [
        "with open(ADWORDS_FILE, \"w\") as adwords_file:\n",
        "    print(adwords_content, file=adwords_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDJ3_QYYS1Ua"
      },
      "outputs": [],
      "source": [
        "# Google Ads client\n",
        "# adwords_client = adwords.AdWordsClient.LoadFromStorage(ADWORDS_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObzXCrly_gZe"
      },
      "source": [
        "### Create an AdWords user list\n",
        "Using emails of the top LTV customers, you create an AdWords list. If more than 5000 of the users are matched with AdWords email, a similar audience list will be created.\n",
        "\n",
        "> Note that this guide uses fake emails so running these steps is not going to work but you can leverage this code with emails coming from your CRM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfBQWgj8HhQQ"
      },
      "outputs": [],
      "source": [
        "ltv_emails = list(set(df_top_ltv[\"email\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaDxOKy7aos"
      },
      "outputs": [],
      "source": [
        "# https://developers.google.com/adwords/api/docs/samples/python/remarketing#create-and-populate-a-user-list\n",
        "# https://github.com/googleads/googleads-python-lib/blob/7c41584c65759b6860572a13bde65d7395c5b2d8/examples/adwords/v201809/remarketing/add_crm_based_user_list.py\n",
        "\n",
        "# \"\"\"Adds a user list and populates it with hashed email addresses.\n",
        "\n",
        "# Note: It may take several hours for the list to be populated with members. Email\n",
        "# addresses must be associated with a Google account. For privacy purposes, the\n",
        "# user list size will show as zero until the list has at least 1000 members. After\n",
        "# that, the size will be rounded to the two most significant digits.\n",
        "# \"\"\"\n",
        "# def normalize_and_SHA256(s):\n",
        "#   \"\"\"Normalizes (lowercase, remove whitespace) and hashes a string with SHA-256.\n",
        "\n",
        "#   Args:\n",
        "#     s: The string to perform this operation on.\n",
        "\n",
        "#   Returns:\n",
        "#     A normalized and SHA-256 hashed string.\n",
        "#   \"\"\"\n",
        "#   return hashlib.sha256(s.strip().lower()).hexdigest()\n",
        "\n",
        "# def create_user_list(client):\n",
        "#   # Initialize appropriate services.\n",
        "#   user_list_service = client.GetService('AdwordsUserListService', 'v201809')\n",
        "\n",
        "#   user_list = {\n",
        "#       'xsi_type': 'CrmBasedUserList',\n",
        "#       'name': f'Customer relationship management list #{uuid.uuid4()}',\n",
        "#       'description': 'A list of customers that originated from email addresses',\n",
        "#       # CRM-based user lists can use a membershipLifeSpan of 10000 to indicate\n",
        "#       # unlimited; otherwise normal values apply.\n",
        "#       'membershipLifeSpan': 30,\n",
        "#       'uploadKeyType': 'CONTACT_INFO'\n",
        "#   }\n",
        "\n",
        "#   # Create an operation to add the user list.\n",
        "#   operations = [{\n",
        "#       'operator': 'ADD',\n",
        "#       'operand': user_list\n",
        "#   }]\n",
        "\n",
        "#   result = user_list_service.mutate(operations)\n",
        "#   user_list_id = result['value'][0]['id']\n",
        "\n",
        "#   emails = ltv_emails\n",
        "#   members = [{'hashedEmail': normalize_and_SHA256(email)} for email in emails]\n",
        "\n",
        "#   mutate_members_operation = {\n",
        "#       'operand': {\n",
        "#           'userListId': user_list_id,\n",
        "#           'membersList': members\n",
        "#       },\n",
        "#       'operator': 'ADD'\n",
        "#   }\n",
        "\n",
        "#   response = user_list_service.mutateMembers([mutate_members_operation])\n",
        "\n",
        "#   if 'userLists' in response:\n",
        "#     for user_list in response['userLists']:\n",
        "#       print('User list with name \"%s\" and ID \"%d\" was added.'\n",
        "#             % (user_list['name'], user_list['id']))\n",
        "\n",
        "# create_user_list(adwords_client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcw1uVqmgdrG"
      },
      "source": [
        "## Thoughts\n",
        "As mentioned a few time, the goal of this tutorial is not to provide a state of the art model for predicting LTV but show a possible approach to create a base model for predicting future monetary value of your customers.\n",
        "\n",
        "If you decided to investigate more the approach, you could:\n",
        "- Do additional data engineering or find additional example when you see outliers.\n",
        "- Try a different approach to create inputs. For example, you could:\n",
        "  - Use a ratio of transactions as *input transactions* and the rest as *target transactions*. An earlier version of this tutorial tried that but the current method led to better results for the problem at hand (predict value in the next X days).\n",
        "  - Customize the time range for input transactions. This tutorial uses all orders that happened before a threshold but you could image to use 6 months before to predict one month after.\n",
        "  - The data preparation creates several examples from a unique customer by using multiple threshold dates. The model can link those example with their original customer. A more advanced approach could try to treat those example as timeseries examples and use how the customer behavior evolve over time. Model might perform better...or not. As you noticed, the `T` value kind of contains that information already across examples that might seem random to the model. Let us know if you decide to try and find some great results.\n",
        "- When comparing models, make sure that you keep a unique test set to fairly compare the model's performance.\n",
        "- This tutorial prepares the data knowing that you want to predict the orders for the next 30 days. If you want a model to predict for the next quarter, you need to update `LENGTH_FUTURE` and prepare a new training dataset.\n",
        "- The original dataset uses a limited number of fields. If you have other dimensions such as product categories, regions or demographics, try to create additional inputs for your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all GCP resources used in this project, you can [delete the GCP\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bqml_automl_ltv_activate_lookalike.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
