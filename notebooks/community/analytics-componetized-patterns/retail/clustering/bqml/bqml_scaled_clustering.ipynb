{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHLV0D7Y5jtU"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/blob/master/retail/clustering/bqml/bqml_scaled_clustering.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "# How to build k-means clustering models for market segmentation using BigQuery ML\n",
        "\n",
        "A common marketing analytics challenge is to understand consumer behavior and develop customer attributes or archetypes.  As organizations get better at tackling this problem, they can activate marketing strategies to incorporate additional customer knowledge into their campaigns.  \n",
        "\n",
        "Clustering algorithms are a common vehicle to address this challenge. They allow businesses to better segment and understand their customers and users.  In the field of Machine Learning, which is a combination of both art and science, unsupervised learning may require more art compared to supervised learning algorithms.  By definition, unsupervised learning has no single metric to guide the algorithm's learning process. Instead, the data science team will need to work hand in hand with business owners to determine feature selection, optimal number of clusters (the number of clusters is often abbreviated as k), and most importantly, to gain a deeper understanding of what each cluster represents.    \n",
        "\n",
        "### How can clustering algorithms help businesses succeed?\n",
        "\n",
        "Clustering algorithms can help companies identify groups of similar customers that can be used for targeting in advertising campaigns. This is paramount as we are breathing a prediction era where customers expect personalization from brands. \n",
        " \n",
        "Using a public sample Google Analytics 360 e-commerce dataset on BigQuery, you will learn how to create and deploy clustering algorithms in production. You will also get an example of how to navigate unsupervised learning.  Keep in mind, your clusters will be even more meaningful when you bring additional data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XbaFW4SQxGD"
      },
      "source": [
        "# Objective\n",
        "\n",
        "By the end of this notebook, you will know how to:\n",
        "* Explore features to understand what might be interesting for a clustering model\n",
        "* Pre-process data into the correct format needed to create a clustering model using BigQuery ML\n",
        "* Train (and deploy) the k-means model in BigQuery ML\n",
        "* Evaluate the model\n",
        "* Make predictions using the model\n",
        "* Write the results to be used for batch prediction, for example, to send ads based on segmentation\n",
        "\n",
        "## Dataset\n",
        "\n",
        "The [Google Analytics Sample](https://console.cloud.google.com/marketplace/details/obfuscated-ga360-data/obfuscated-ga360-data?filter=solution-type:dataset) dataset, which is hosted publicly on BigQuery, is a dataset that provides 12 months (August 2016 to August 2017) of obfuscated Google Analytics 360 data from the [Google Merchandise Store](https://www.googlemerchandisestore.com/), a real e-commerce store that sells Google-branded merchandise.\n",
        "\n",
        "\n",
        "## Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud Platform:\n",
        "\n",
        "* BigQuery\n",
        "* BigQuery ML\n",
        "\n",
        "Learn about [BigQuery pricing](https://cloud.google.com/bigquery/pricing), [BigQuery ML\n",
        "pricing](https://cloud.google.com/bigquery-ml/pricing) and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## PIP install packages and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b420d0ecff59"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-bigquery\n",
        "!pip install google-cloud-bigquery-storage\n",
        "!pip install pandas-gbq\n",
        "\n",
        "# Reservation package needed to setup flex slots for flat-rate pricing\n",
        "!pip install google-cloud-bigquery-reservation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b56be1e7fb84"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud Platform project\n",
        "\n",
        "_The following steps are required, regardless of your notebook environment._\n",
        "\n",
        "1. [Select or create a project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "1. [Enable the AI Platform APIs and Compute Engine APIs.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component)\n",
        "\n",
        "1. Enter your project ID and region in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "_Note_: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8SLpZgiyMSP"
      },
      "source": [
        "### Set project ID and authenticate\n",
        "\n",
        "Update your Project ID below.  The rest of the notebook will run using these credentials. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f7921568a02"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"UPDATE TO YOUR PROJECT ID\"\n",
        "REGION = \"US\"\n",
        "DATA_SET_ID = \"bqml_kmeans\"  # Ensure you first create a data set in BigQuery\n",
        "!gcloud config set project $PROJECT_ID\n",
        "# If you have not built the Data Set, the following command will build it for you\n",
        "# !bq mk --location=$REGION --dataset $PROJECT_ID:$DATA_SET_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbnrwv-nyi82"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_gbq\n",
        "from google.cloud import bigquery\n",
        "\n",
        "pd.set_option(\n",
        "    \"display.float_format\", lambda x: \"%.3f\" % x\n",
        ")  # used to display float format\n",
        "client = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4xTsQJk1Ia"
      },
      "source": [
        "# Data exploration and preparation\n",
        "\n",
        "Prior to building your models, you are typically expected to invest a significant amount of time cleaning, exploring, and aggregating your dataset in a meaningful way for modeling.  For the purpose of this demo, we aren't showing this step only to prioritize showcasing clustering with k-means in BigQuery ML.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3NG3XuNYKnO"
      },
      "source": [
        "## Building synthetic data\n",
        "\n",
        "Our goal is to use both online (GA360) and offline (CRM) data.  You can use your own CRM data, however, in this case since we don't have CRM data to showcase, we will instead generate synthetic data.  We will generate estimated House Hold Income, and Gender.  To do so, we will hash fullVisitorID and build simple rules based on the last digit of the hash. When you run this process with your own data, you can join CRM data with several dimensions, but this is just an example of what is possible.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff235a6600ba"
      },
      "outputs": [],
      "source": [
        "# We start with GA360 data, and will eventually build synthetic CRM as an example.\n",
        "# This block is the first step, just working with GA360\n",
        "\n",
        "ga360_only_view = \"GA360_View\"\n",
        "shared_dataset_ref = client.dataset(DATA_SET_ID)\n",
        "ga360_view_ref = shared_dataset_ref.table(ga360_only_view)\n",
        "ga360_view = bigquery.Table(ga360_view_ref)\n",
        "\n",
        "ga360_query = \"\"\"\n",
        "SELECT\n",
        "  fullVisitorID,\n",
        "  ABS(farm_fingerprint(fullVisitorID)) AS Hashed_fullVisitorID, # This will be used to generate random data.\n",
        "  MAX(device.operatingSystem) AS OS, # We can aggregate this because an OS is tied to a fullVisitorID.\n",
        "  SUM (CASE\n",
        "       WHEN REGEXP_EXTRACT (v2ProductCategory, \n",
        "                           r'^(?:(?:.*?)Home/)(.*?)/') \n",
        "                           = 'Apparel' THEN 1 ELSE 0 END) AS Apparel,\n",
        "  SUM (CASE  \n",
        "       WHEN REGEXP_EXTRACT (v2ProductCategory, \n",
        "                           r'^(?:(?:.*?)Home/)(.*?)/') \n",
        "                           = 'Office' THEN 1 ELSE 0 END) AS Office,\n",
        "  SUM (CASE\n",
        "       WHEN REGEXP_EXTRACT (v2ProductCategory, \n",
        "                           r'^(?:(?:.*?)Home/)(.*?)/') \n",
        "                           = 'Electronics' THEN 1 ELSE 0 END) AS Electronics,\n",
        "  SUM (CASE\n",
        "       WHEN REGEXP_EXTRACT (v2ProductCategory, \n",
        "                           r'^(?:(?:.*?)Home/)(.*?)/') \n",
        "                           = 'Limited Supply' THEN 1 ELSE 0 END) AS LimitedSupply,\n",
        "  SUM (CASE\n",
        "       WHEN REGEXP_EXTRACT (v2ProductCategory, \n",
        "                           r'^(?:(?:.*?)Home/)(.*?)/') \n",
        "                           = 'Accessories' THEN 1 ELSE 0 END) AS Accessories,\n",
        "  SUM (CASE\n",
        "       WHEN REGEXP_EXTRACT (v2ProductCategory, \n",
        "                           r'^(?:(?:.*?)Home/)(.*?)/') \n",
        "                           = 'Shop by Brand' THEN 1 ELSE 0 END) AS ShopByBrand,\n",
        "  SUM (CASE\n",
        "       WHEN REGEXP_EXTRACT (v2ProductCategory, \n",
        "                           r'^(?:(?:.*?)Home/)(.*?)/') \n",
        "                           = 'Bags' THEN 1 ELSE 0 END) AS Bags,\n",
        "  ROUND (SUM (productPrice/1000000),2) AS productPrice_USD\n",
        "FROM\n",
        "  `bigquery-public-data.google_analytics_sample.ga_sessions_*`,\n",
        "  UNNEST(hits) AS hits,\n",
        "  UNNEST(hits.product) AS hits_product\n",
        "WHERE\n",
        "  _TABLE_SUFFIX BETWEEN '20160801'\n",
        "  AND '20160831'\n",
        "  AND geoNetwork.country = 'United States'\n",
        "  AND type = 'EVENT'\n",
        "GROUP BY\n",
        "  1,\n",
        "  2\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ga360_view.view_query = ga360_query.format(PROJECT_ID)\n",
        "ga360_view = client.create_table(ga360_view)  # API request\n",
        "\n",
        "print(f\"Successfully created view at {ga360_view.full_table_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9845cd1c379a"
      },
      "outputs": [],
      "source": [
        "# Show a sample of GA360 data\n",
        "\n",
        "ga360_query_df = f\"\"\"\n",
        "SELECT * FROM {ga360_view.full_table_id.replace(\":\", \".\")} LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "\n",
        "# Start the query\n",
        "query_job = client.query(ga360_query_df, job_config=job_config)  # API Request\n",
        "df_ga360 = query_job.result()\n",
        "df_ga360 = df_ga360.to_dataframe()\n",
        "\n",
        "df_ga360"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f932026ce42"
      },
      "outputs": [],
      "source": [
        "# Create synthetic CRM data in SQL\n",
        "\n",
        "CRM_only_view = \"CRM_View\"\n",
        "shared_dataset_ref = client.dataset(DATA_SET_ID)\n",
        "CRM_view_ref = shared_dataset_ref.table(CRM_only_view)\n",
        "CRM_view = bigquery.Table(CRM_view_ref)\n",
        "\n",
        "# Query below works by hashing the fullVisitorID, which creates a random distribution.\n",
        "# We use modulo to artificially split gender and hhi distribution.\n",
        "CRM_query = \"\"\"\n",
        "SELECT\n",
        "  fullVisitorID,\n",
        "IF\n",
        "  (MOD(Hashed_fullVisitorID,2) = 0,\n",
        "    \"M\",\n",
        "    \"F\") AS gender,\n",
        "  CASE\n",
        "    WHEN MOD(Hashed_fullVisitorID,10) = 0 THEN 55000\n",
        "    WHEN MOD(Hashed_fullVisitorID,10) < 3 THEN 65000\n",
        "    WHEN MOD(Hashed_fullVisitorID,10) < 7 THEN 75000\n",
        "    WHEN MOD(Hashed_fullVisitorID,10) < 9 THEN 85000\n",
        "    WHEN MOD(Hashed_fullVisitorID,10) = 9 THEN 95000\n",
        "  ELSE\n",
        "  Hashed_fullVisitorID\n",
        "END\n",
        "  AS hhi\n",
        "FROM (\n",
        "  SELECT\n",
        "    fullVisitorID,\n",
        "    ABS(farm_fingerprint(fullVisitorID)) AS Hashed_fullVisitorID,\n",
        "  FROM\n",
        "    `bigquery-public-data.google_analytics_sample.ga_sessions_*`,\n",
        "    UNNEST(hits) AS hits,\n",
        "    UNNEST(hits.product) AS hits_product\n",
        "  WHERE\n",
        "    _TABLE_SUFFIX BETWEEN '20160801'\n",
        "    AND '20160831'\n",
        "    AND geoNetwork.country = 'United States'\n",
        "    AND type = 'EVENT'\n",
        "  GROUP BY\n",
        "    1,\n",
        "    2)\n",
        "\"\"\"\n",
        "\n",
        "CRM_view.view_query = CRM_query.format(PROJECT_ID)\n",
        "CRM_view = client.create_table(CRM_view)  # API request\n",
        "\n",
        "print(f\"Successfully created view at {CRM_view.full_table_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d19a3443e1c5"
      },
      "outputs": [],
      "source": [
        "# See an output of the synthetic CRM data\n",
        "\n",
        "CRM_query_df = f\"\"\"\n",
        "SELECT * FROM {CRM_view.full_table_id.replace(\":\", \".\")} LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "\n",
        "# Start the query\n",
        "query_job = client.query(CRM_query_df, job_config=job_config)  # API Request\n",
        "df_CRM = query_job.result()\n",
        "df_CRM = df_CRM.to_dataframe()\n",
        "\n",
        "df_CRM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7703eada71ae"
      },
      "source": [
        "## Build a final view for to use as trainding data for clustering\n",
        "\n",
        "You may decide to change the view below based on your specific dataset.  This is fine, and is exactly why we're creating a view.  All steps subsequent to this will reference this view.  If you change the SQL below, you won't need to modify other parts of the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a00506f6babe"
      },
      "outputs": [],
      "source": [
        "# Build a final view, which joins GA360 data with CRM data\n",
        "\n",
        "final_data_view = \"Final_View\"\n",
        "shared_dataset_ref = client.dataset(DATA_SET_ID)\n",
        "final_view_ref = shared_dataset_ref.table(final_data_view)\n",
        "final_view = bigquery.Table(final_view_ref)\n",
        "\n",
        "final_data_query = f\"\"\"\n",
        "SELECT\n",
        "    g.*,\n",
        "    c.* EXCEPT(fullVisitorId)\n",
        "FROM {ga360_view.full_table_id.replace(\":\", \".\")} g\n",
        "JOIN {CRM_view.full_table_id.replace(\":\", \".\")} c\n",
        "ON g.fullVisitorId = c.fullVisitorId\n",
        "\"\"\"\n",
        "\n",
        "final_view.view_query = final_data_query.format(PROJECT_ID)\n",
        "final_view = client.create_table(final_view)  # API request\n",
        "\n",
        "print(f\"Successfully created view at {final_view.full_table_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "185b07b78f8f"
      },
      "outputs": [],
      "source": [
        "# Show final data used prior to modeling\n",
        "\n",
        "sql_demo = f\"\"\"\n",
        "SELECT * FROM {final_view.full_table_id.replace(\":\", \".\")} LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "\n",
        "# Start the query\n",
        "query_job = client.query(sql_demo, job_config=job_config)  # API Request\n",
        "df_demo = query_job.result()\n",
        "df_demo = df_demo.to_dataframe()\n",
        "\n",
        "df_demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpGc5AggVOBN"
      },
      "source": [
        "# Create our initial model\n",
        "\n",
        "In this section, we will build our initial k-means model.  We won't focus on optimal k or other hyperparemeters just yet.\n",
        "\n",
        "Some additional points:  \n",
        "\n",
        "1. We remove fullVisitorId as an input, even though it is grouped at that level because we don't need fullVisitorID as a feature for clustering. fullVisitorID should never be used as feature.\n",
        "2. We have both categorical as well as numerical features\n",
        "3. We do not have to normalize any numerical features, as BigQuery ML will automatically do this for us. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyOB3FePfiIe"
      },
      "source": [
        "## Build a function to build our model\n",
        "\n",
        "We will build a simple python function to build our model, rather than doing everything in SQL.  This approach means we can asynchronously start several models and let BQ run in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "606146ec72b5"
      },
      "outputs": [],
      "source": [
        "def makeModel(n_Clusters, Model_Name):\n",
        "    sql = f\"\"\"\n",
        "    CREATE OR REPLACE MODEL `{PROJECT_ID}.{DATA_SET_ID}.{Model_Name}` \n",
        "    OPTIONS(model_type='kmeans',\n",
        "    kmeans_init_method = 'KMEANS++',\n",
        "    num_clusters={n_Clusters}) AS\n",
        "\n",
        "    SELECT * except(fullVisitorID, Hashed_fullVisitorID) FROM `{final_view.full_table_id.replace(\":\", \".\")}`\n",
        "    \"\"\"\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    client.query(sql, job_config=job_config)  # Make an API request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDUIzlcYyMOt"
      },
      "outputs": [],
      "source": [
        "# Let's start with a simple test to ensure everything works.\n",
        "# After running makeModel(), allow a few minutes for training to complete.\n",
        "\n",
        "model_test_name = \"test\"\n",
        "makeModel(3, model_test_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ee6aeed72f8"
      },
      "outputs": [],
      "source": [
        "# After training is completed, you can either check in the UI, or you can interact with it using list_models().\n",
        "\n",
        "for model in client.list_models(DATA_SET_ID):\n",
        "    print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2mDX1-WZg8k"
      },
      "source": [
        "# Work towards creating a better model\n",
        "\n",
        "In this section, we want to determine the proper k value.  Determining the right value of k depends completely on the use case.  There are straight forward examples that will simply tell you how many clusters are needed.  Suppose you are pre-processing hand written digits - this tells us k should be 10. Or perhaps your business stakeholder only wants to deliver three different marketing campaigns and needs you to identify three clusters of customers, then setting k=3 might be meaningful. However, the use case is sometimes more open ended and you may want to explore different numbers of clusters to see how your datapoints group together with the minimal error within each cluster. To accomplish this process, we start by performing the 'Elbow Method', which simply charts loss vs k. Then, we'll also use the Davies-Bouldin score.\n",
        "(https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajkLrlFjlvPQ"
      },
      "source": [
        "Below we are going to create several models to perform both the Elbow Method and get the Davies-Bouldin score.  You may change parameters like low_k and high_k. Our process will create models between these two values.  There is an additional parameter called model_prefix_name.  We recommend you leave this as its current value.  It is used to generate a naming convention for our models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAuyizlkzzQU"
      },
      "outputs": [],
      "source": [
        "# Define upper and lower bound for k, then build individual models for each.\n",
        "# After running this loop, look at the UI to see several model objects that exist.\n",
        "\n",
        "low_k = 3\n",
        "high_k = 15\n",
        "model_prefix_name = \"kmeans_clusters_\"\n",
        "\n",
        "lst = list(range(low_k, high_k + 1))  # build list to iterate through k values\n",
        "\n",
        "for k in lst:\n",
        "    model_name = model_prefix_name + str(k)\n",
        "    makeModel(k, model_name)\n",
        "    print(f\"Model started: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVxjQFWmVVmH"
      },
      "source": [
        "## Select optimal k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp7y6mksNY4D"
      },
      "outputs": [],
      "source": [
        "# list all current models\n",
        "models = client.list_models(DATA_SET_ID)  # Make an API request.\n",
        "print(\"Listing current models:\")\n",
        "for model in models:\n",
        "    full_model_id = f\"{model.dataset_id}.{model.model_id}\"\n",
        "    print(full_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC8GAkKxhN9B"
      },
      "outputs": [],
      "source": [
        "# Remove our sample model from BigQuery, so we only have remaining models from our previous loop\n",
        "\n",
        "model_id = DATA_SET_ID + \".\" + model_test_name\n",
        "client.delete_model(model_id)  # Make an API request.\n",
        "print(f\"Deleted model '{model_id}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnGjnnrvH2Ej"
      },
      "outputs": [],
      "source": [
        "# This will create a dataframe with each model name, the Davies Bouldin Index, and Loss.\n",
        "# It will be used for the elbow method and to help determine optimal K\n",
        "\n",
        "df = pd.DataFrame(columns=[\"davies_bouldin_index\", \"mean_squared_distance\"])\n",
        "models = client.list_models(DATA_SET_ID)  # Make an API request.\n",
        "for model in models:\n",
        "    full_model_id = f\"{model.dataset_id}.{model.model_id}\"\n",
        "    sql = f\"\"\"\n",
        "        SELECT \n",
        "            davies_bouldin_index,\n",
        "            mean_squared_distance \n",
        "        FROM ML.EVALUATE(MODEL `{full_model_id}`)\n",
        "    \"\"\"\n",
        "\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "\n",
        "    # Start the query, passing in the extra configuration.\n",
        "    query_job = client.query(sql, job_config=job_config)  # Make an API request.\n",
        "    df_temp = query_job.to_dataframe()  # Wait for the job to complete.\n",
        "    df_temp[\"model_name\"] = model.model_id\n",
        "    df = pd.concat([df, df_temp], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLBMjSR-qqCq"
      },
      "source": [
        "The code below assumes we've used the naming convention originally created in this notebook, and the k value occurs after the 2nd underscore.  If you've changed the model_prefix_name variable, then this code might break. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DSIBlqVahZ7"
      },
      "outputs": [],
      "source": [
        "# This will modify the dataframe above, produce a new field with 'n_clusters', and will sort for graphing\n",
        "\n",
        "df[\"n_clusters\"] = df[\"model_name\"].str.split(\"_\").map(lambda x: x[2])\n",
        "df[\"n_clusters\"] = df[\"n_clusters\"].apply(pd.to_numeric)\n",
        "df = df.sort_values(by=\"n_clusters\", ascending=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLVVMKm8QIFv"
      },
      "outputs": [],
      "source": [
        "df.plot.line(x=\"n_clusters\", y=[\"davies_bouldin_index\", \"mean_squared_distance\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKWIJMXBcatm"
      },
      "source": [
        "Note - when you run this notebook, you will get different results, due to random cluster initialization.  If you'd like to consistently return the same cluster for reach run, you may explicitly select your initialization through hyperparameter selection (https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#kmeans_init_method). \n",
        "\n",
        "Making our k selection:  There is no perfect approach or process when determining the optimal k value. It can often be determined by business rules or requirements.  In this example, there isn't a simple requirement, so these considerations can also be followed:\n",
        "\n",
        "\n",
        "1.   We start with the 'elbow method', which is effectively charting loss vs k.  Sometimes, though not always, there's a natural 'elbow' where incremental clusters do not drastically reduce loss.  In this specific example, and as you often might find, unfortunately there isn't a natural 'elbow', so we must continue our process. \n",
        "2.  Next, we chart Davies-Bouldin vs k.  This score tells us how 'different' each cluster is, with the optimal score at zero.  With 5 clusters, we see a score of ~1.4, and only with k>9, do we see better values. \n",
        "3. Finally, we begin to try to interpret the difference of each model. You can review the evaluation module for various models to understand distributions of our features.  With our data, we can look for patterns by gender, house hold income, and shopping habits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aUBVjqsy3Lo"
      },
      "source": [
        "# Analyze our final cluster\n",
        "\n",
        "There are 2 options to understand the characteristics of your model.  You can either 1) look in the BigQuery UI, or you can 2) programmatically interact with your model object.  Below you’ll find a simple example for the latter option.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo6wjkebuOVB"
      },
      "outputs": [],
      "source": [
        "model_to_use = \"kmeans_clusters_5\"  # User can edit this\n",
        "final_model = DATA_SET_ID + \".\" + model_to_use\n",
        "\n",
        "sql_get_attributes = f\"\"\"\n",
        "SELECT\n",
        "  centroid_id,\n",
        "  feature,\n",
        "  categorical_value\n",
        "FROM\n",
        "  ML.CENTROIDS(MODEL {final_model})\n",
        "WHERE\n",
        "  feature IN ('OS','gender')\n",
        "\"\"\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "\n",
        "# Start the query\n",
        "query_job = client.query(sql_get_attributes, job_config=job_config)  # API Request\n",
        "df_attributes = query_job.result()\n",
        "df_attributes = df_attributes.to_dataframe()\n",
        "df_attributes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "084e0b168453"
      },
      "outputs": [],
      "source": [
        "# get numerical information about clusters\n",
        "\n",
        "sql_get_numerical_attributes = f\"\"\"\n",
        "WITH T AS (\n",
        "SELECT \n",
        "  centroid_id,\n",
        "  ARRAY_AGG(STRUCT(feature AS name, \n",
        "                   ROUND(numerical_value,1) AS value) \n",
        "                   ORDER BY centroid_id) \n",
        "                   AS cluster\n",
        "FROM ML.CENTROIDS(MODEL {final_model})\n",
        "GROUP BY centroid_id\n",
        "),\n",
        "\n",
        "Users AS(\n",
        "SELECT\n",
        "  centroid_id,\n",
        "  COUNT(*) AS Total_Users\n",
        "FROM(\n",
        "SELECT\n",
        "  * EXCEPT(nearest_centroids_distance)\n",
        "FROM\n",
        "  ML.PREDICT(MODEL {final_model},\n",
        "    (\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      {final_view.full_table_id.replace(\":\", \".\")}\n",
        "      )))\n",
        "GROUP BY centroid_id\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  centroid_id,\n",
        "  Total_Users,\n",
        "  (SELECT value from unnest(cluster) WHERE name = 'Apparel') AS Apparel,\n",
        "  (SELECT value from unnest(cluster) WHERE name = 'Office') AS Office,\n",
        "  (SELECT value from unnest(cluster) WHERE name = 'Electronics') AS Electronics,\n",
        "  (SELECT value from unnest(cluster) WHERE name = 'LimitedSupply') AS LimitedSupply,\n",
        "  (SELECT value from unnest(cluster) WHERE name = 'Accessories') AS Accessories,\n",
        "  (SELECT value from unnest(cluster) WHERE name = 'ShopByBrand') AS ShopByBrand,\n",
        "  (SELECT value from unnest(cluster) WHERE name = 'Bags') AS Bags,\n",
        "  (SELECT value from unnest(cluster) WHERE name = 'productPrice_USD') AS productPrice_USD,\n",
        "  (SELECT value from unnest(cluster) WHERE name = 'hhi') AS hhi\n",
        "\n",
        "FROM T LEFT JOIN Users USING(centroid_id)\n",
        "ORDER BY centroid_id ASC\n",
        "\"\"\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "\n",
        "# Start the query\n",
        "query_job = client.query(\n",
        "    sql_get_numerical_attributes, job_config=job_config\n",
        ")  # API Request\n",
        "df_numerical_attributes = query_job.result()\n",
        "df_numerical_attributes = df_numerical_attributes.to_dataframe()\n",
        "df_numerical_attributes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t95SlHjlx3Qn"
      },
      "source": [
        "In addition to the output above, I'll note a few insights we get from our clusters. \n",
        "\n",
        "Cluster 1 - The apparel shopper, which also purchases more often than normal.  This (although synthetic data) segment skews female.\n",
        "\n",
        "Cluster 2 - Most likely to shop by brand, and interested in bags. This segment has fewer purchases on average than the first cluster, however, this is the highest value customer.\n",
        "\n",
        "Cluster 3 - The most populated cluster, this one has a small amount of purchases and spends less on average.  This segment is the one time buyer, rather than the brand loyalist.  \n",
        "\n",
        "Cluster 4 - Most interested in accessories, does not buy as often as cluster 1 and 2, however buys more than cluster 3.  \n",
        "\n",
        "Cluster 5 - This is an outlier as only 1 person belongs to this group.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv0hMz3rcb1s"
      },
      "source": [
        "# Use model to group new website behavior, and then push results to GA360 for marketing activation\n",
        "\n",
        "After we have a finalized model, we want to use it for inference.  The code below outlines how to score or assign users into clusters.  These are labeled as the CENTROID_ID.  Although this by itself is helpful, we also recommend a process to ingest these scores back into GA360. The easiest way to export your BigQuery ML predictions from a BigQuery table to Google Analytics 360 is to use the MoDeM (Model Deployment for Marketing, https://github.com/google/modem) reference implementation. MoDeM helps you load data into Google Analytics for eventual activation in Google Ads, Display & Video 360 and Search Ads 360."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fAsssnWnv5C"
      },
      "outputs": [],
      "source": [
        "sql_score = f\"\"\"\n",
        "SELECT * EXCEPT(nearest_centroids_distance)\n",
        "FROM\n",
        "  ML.PREDICT(MODEL {final_model},\n",
        "    (\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      {final_view.full_table_id.replace(\":\", \".\")}\n",
        "      LIMIT 1))\n",
        "\"\"\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "\n",
        "# Start the query\n",
        "query_job = client.query(sql_score, job_config=job_config)  # API Request\n",
        "df_score = query_job.result()\n",
        "df_score = df_score.to_dataframe()\n",
        "\n",
        "df_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnHw5kQUYfkK"
      },
      "source": [
        "# Clean up: Delete all models and tables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ8VwVcMYlW7"
      },
      "outputs": [],
      "source": [
        "# Are you sure you want to do this? This is to delete all models\n",
        "\n",
        "models = client.list_models(DATA_SET_ID)  # Make an API request.\n",
        "for model in models:\n",
        "    full_model_id = f\"{model.dataset_id}.{model.model_id}\"\n",
        "    client.delete_model(full_model_id)  # Make an API request.\n",
        "    print(f\"Deleted: {full_model_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAXrHpAJYwCI"
      },
      "outputs": [],
      "source": [
        "# Are you sure you want to do this? This is to delete all tables and views\n",
        "\n",
        "tables = client.list_tables(DATA_SET_ID)  # Make an API request.\n",
        "for table in tables:\n",
        "    full_table_id = f\"{table.dataset_id}.{table.table_id}\"\n",
        "    client.delete_table(full_table_id)  # Make an API request.\n",
        "    print(f\"Deleted: {full_table_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYucZEztyk2K"
      },
      "source": [
        "# Wrapping it all up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfzOUkdiytXm"
      },
      "source": [
        "In this exercise, we’ve accomplished some cool things with k-means in BigQuery ML.  Most notably, we’re able to join online and offline user level information to gain more insight into a holistic view of our customers.  We’ve modeled user behavior, and detailed an approach to determine the optimal number of clusters.  We’re able to take this insight and apply to future behavior through inference.  Finally, we can import this inference score back into GA360 for future marketing campaigns. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bqml_scaled_clustering.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
